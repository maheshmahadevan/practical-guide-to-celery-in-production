[00:00:00] Next up we have a talk by Mahesh, who is a DevOps and Engineering Lead at Avoma. He likes to do innovations in SaaS and intelligence, so he'll be talking about a practical guide to Celery.

[00:00:17] Thank you for the introduction. I'll start with a small introduction of myself and Avoma, then we'll go to the rest of the slides. I'm a Principal Software Engineer. I have been in Python and Celery for the past 2 years. Avoma is an AI meeting assistant. We use AI meeting assistant to accelerate revenue for sales and other teams. I'll start with basics of Django and Celery, then go deeper into how we run Celery in production, deployment and monitoring of Celery, and go with the war-time stories that we have had and the current challenges.

[00:01:23] Quick show of hands: how many of you use Django and Celery in production? Awesome. Most people know here that Django is an all-batteries-included framework. Most of the top companies use Django for fast prototyping and getting from concept to production as fast as possible. We also use it as a backbone. Celery helps to offload most of the time-insensitive tasks. Long-running tasks could be offloaded to an async worker. It's basically a Unix async worker. We offload all of our email events, video processing events, or report events. All of them we move to Celery workers.

[00:02:23] Celery works on AMQP, which is short for Advanced Message Queuing Protocol. Some of the frameworks that use AMQP you might know are RabbitMQ, Redis, or SQS. We ourselves use SQS, AWS SQS for running Celery because we are heavily deployed on AWS.

[00:02:50] The aim of this talk is a practical guide to running Celery in production. We want to bring forward our experiences of running Celery at scale in production. I hope some of our experience can help you understand what you could do to run Celery in production. I'm going to start with a simplified view of how we run Celery, then I'll probably go into a more complex view.

[00:03:27] As I mentioned, in Celery itself, it runs on AMQP. It requires some sort of a broker, which in this case is SQS for us. A task starts from Django or Python code, which then comes to SQS. On the other side of things, there are Celery workers, which in our case is a bunch of EC2 workers. These EC2 workers are sitting and listening for any new message that are coming on the broker. They accept it, they process it, and they go forward.

[00:03:58] One of the interesting things that we have done with our Auto Scaling groups and the workers is all our workers run as spot instances, obviously for cost optimization purposes. For folks who are uninitiated and don't know what spot instances are: spot instances are workers that you could purchase from AWS at about 70% discounted rate because they are just lying around and nobody's asking for them. It's not on demand. It helps us manage the cost but also run Celery and async tasks at scale. We use Auto Scaling groups.

[00:04:52] Coming back to the complex view of how exactly we run Celery: having just one queue and a number of tasks is not going to help you achieve the parallelism that you want to achieve. What we have done is we have differentiated or bucketed each of our tasks into different queues. The way we bucket them is by functionality or the time it takes to run and also the processing power that it needs.

[00:05:17] For example, all of our video tasks go into one queue, all of our email tasks go to another queue, transcripts go into another queue. It helps us parallelize the messages and the task itself. Video workers are heavily powered. They have much better CPU and RAM, for example, and they're fine-tuned to process video tasks, whereas email or some of the report generation, they do not need that kind of power. That helps us scale differently. Even the concurrency is managed separately.

[00:06:01] The mapping of the queues to the workers itself is done by tags. We use the tags in AWS to tell Celery when it starts up it needs to listen to this particular queue. For example, whenever in Auto Scaling group that is particular to video, we tell them that this particular set of workers need to listen to video queue. As soon as Celery starts up, it starts listening to all the messages from the queue and that's how it starts processing. This is how we have managed the parallelism and separated the tasks.

[00:06:44] One more interesting aspect is to manage different concurrency levels for different Celery tasks. Some of you who have already worked with Celery might know that you could manage concurrency differently. We have managed concurrency differently by also using tags. When Celery starts up, you could have tags. For example, video workers have concurrency of one. Every worker at any point in time only processes one particular task, whereas email, transcript, or a few of our other jobs, they could probably have multiple different concurrency. It's managed differently based on the workloads that we operate.

[00:07:24] As I mentioned, scaling of spot instances or instances in general is going to be a challenge. There are two particular reasons why we want to do it: obviously want to manage cost, and you don't want to have that many number of instances running if there are not many tasks. Given the load at which we operate, it keeps changing based on the number of events that come because the tasks themselves are bracketed in the end time.

[00:07:50] Whenever a meeting ends in a particular time period, you will see there are heavy surges of tasks that come through, whereas during non-linear hours or India time hours, you won't see as much load. We have divided our scaling into three different patterns. One is based on time zone. During the US time zone or the peak loads, we scale up the number of workers. That's called the schedule scaling policy. During the India time zone, scale it down.

[00:08:28] The second one is based on the queue depth. Queue depth is basically just understanding how many number of messages are present on the queue at any point in time. Looking towards the number of visible messages or non-visible messages, we try to scale it out or scale it in.

[00:08:47] The third one, which has actually given us much more benefit, is to use predictive scaling. Predictive scaling is an AWS-specific feature which basically keeps learning through the queue sizes for a period of time. It has its own machine learning model to understand at what point in time do you see normally your peaks and what point in time you don't have your peaks. Based on that, it applies the scaling automatically 5 minutes before the peak is about to start. It helps to not have that warm or the cold start time. Your machines, the workers, are up and ready for tasks to be consumed, and you don't have as much wait time as expected.

[00:09:45] Some of the challenges that we had specifically in scaling are due to spot scaling. One of the disadvantages: even if they give you at a lower cost, the spot workers could be reclaimed by AWS at any point in time. They will give you a two-minute window and they could just go ahead and terminate. That's how they manage their cost. Having said that, you need to gracefully shut down Celery whenever such an event comes through.

[00:10:19] There are a number of ways how we have managed it. One of the ways is using simply using a cron script to keep looking at what point in time a spot termination has been invoked at a particular worker. When something like that happens, we tell the Celery to stop taking any more messages from the queue and gracefully shut down, and it doesn't accept any more messages.

[00:10:51] The other challenge that we had specifically is during scaling in. Since we manage our own scaling-in policy, that could also mean that you might have an existing worker that is processing. You need to tell that worker that Auto Scaling group has already initiated a scaling on this particular instance; you need to gracefully shut down.

[00:11:12] One of the challenges that we had: earlier on we were using something similar like cron. We were using AWS APIs to identify if Auto Scaling group has initiated the scale-down event, but we started hitting the AWS API rate limits. The other advantage we had was since we are on AWS, we started sending the Auto Scaling groups events on scale-in to EventBridge. From EventBridge we sent the information to Lambda, and Lambda invoked an SSM message to the instance itself to start gracefully shutting down Celery workers. This helped us manage the scaling, otherwise which was a challenge for us because managing scale-in would possibly make any of the messages abruptly not process.

[00:12:12] Quick show of the stats or the volume at which we operate currently in production: number of unique Celery tasks that we have is greater than 150. We have managed the queues and Auto Scaling groups to divide them into 50 each. We have one-to-one mapping now altogether, so around 50 queues to around 50 Auto Scaling groups. Number of messages that are during peak comes to around 20K at any instant of time. Total messages that are on queue in a 24-hour period would be somewhere around 100K to 1 million. Total number of workers which process all of these could scale from 100 to 300.

[00:13:06] I'll quickly move to the second slide to describe how our deployment works. What we have done is we have used Fabric for deployment. For folks who don't know what Fabric is: Fabric is a Python-based framework which helps run remote SSH commands. To manage our deployments, we had to make use of Fabric and CodeDeploy in tandem.

[00:13:35] One of our earlier methods of doing deployment was sequential. It basically just built artifacts and used to just send SSH remote commands to deploy on each of these machines. Obviously, as we scaled up, the number of workers started growing. This became quite time-consuming, and we started seeing the time taken for deployment went up to 2 hours, which was not something we could afford because we normally did multiple deployments during the day and we wanted faster deployments.

[00:14:08] The way we managed it is using Fabric. We continued to use Fabric, but in tandem we started using CodeDeploy for it. If you see the sequence of events: Fabric pushes the artifacts to S3, and each of our artifacts are versioned. They're tagged, they're versioned. In parallel, we send to all our Auto Scaling groups a message saying that the new artifacts are uploaded; go ahead and start the deployment for the same. CodeDeploy then initiates deployment in parallel to each of these Auto Scaling groups. They pull these versioned artifacts from S3 and they start parallel deployments of each of these workers. They could be done half at a time interval or all at a time or one at a time based on the nature of each of these workers itself. It brought down the time of deployment from 2 hours to around 5 to 7 minutes.

[00:15:15] One more thing that I wanted to show: along with that, we also had to build our own monitoring tool in order to understand how many of these workers have been actually deployed versus how many are still getting deployed because there was no way for us to reconcile that deployment has finished.

[00:15:39] The way we simply did it is using Grafana and dashboard. What you see is a Grafana dashboard showing the number of workers that are out there running and how many of them have been actually deployed. It's a simple trick. After the deployment, we just tag that particular worker with the version number of the deployment. Then we simply use something like Athena database just to pull in the information and just match it saying that if this worker is actually deployed with this particular version or not. It helped us reconcile the differences. Otherwise, there were cases earlier on where people would complain that even after the deployment, things have not taken effect, and that was basically because the deployment did not go through for one reason or the other. This helped us reconcile those differences.

[00:16:32] Going towards the monitoring aspect of Celery: obviously with so many workers and queues, it becomes important to monitor how things are going through, to understand if queues are getting drained at specific intervals or not. We have used multiple tools. We had explored some of the tools that are already there, for example, Celery Prometheus, Flower, but most of those tools did not work for us.

[00:17:00] One of the challenges with most of these tools were they don't support SQS. We eventually ruled out all of these and we ended up building our own dashboards in order to support this. One of the dashboards that you see on the top corner: it's a side-by-side difference. On the left side you see the data that is being pulled from AWS of the queue. It shows number of messages on the queue, the age of the oldest message. On the right side, what you see is the Auto Scaling group: how it scaled up and scaled down.

[00:17:37] You would know that based on the number of messages, how they are draining. If there are any messages that are not draining correctly, on the right side you see: has your scaling events been done correctly? Are you scaling it up correctly or are you scaling it down appropriately? We also add alerts on queue depths. At any point in time, they're different for different queues.

[00:17:55] For example, one of our major, one of the important queues that we monitor is the time that it takes for our meeting bot to join. The threshold that we have kept on that queue is around 180 seconds. There should be at no point in time that the age of oldest message in that particular queue goes beyond 180. It just sends an instant alert to us knowing that something's going wrong, it's not draining correctly. The other one is queue depth. Obviously, if the threshold is that there are a number of messages in that particular queue go beyond a particular limit, you know that it's not being drained correctly; there's something going wrong around that. Some of these alerts have helped us.

[00:18:45] We also try and monitor the P50, P99, and the max time that a Celery event takes. It helps us evaluate: do we need to change our strategy on how we bucket these tasks into separate queues? Because based on the time that it takes, the max time or P99 time that it takes for each of these tasks, we divide them into different queues.

[00:19:25] Coming to the fun part: production stories. I went into a little bit of detail on each of these, how we manage some of these challenges. The first challenges that I talked about is scaling obviously. We managed scaling with time-based and predictive. Specifically, I'll emphasize on predictive scaling that has helped us improve our scaling and manage cost very effectively. I would recommend everybody to just go and try it.

[00:19:56] The second one was deployment time, which I went into details as to how we brought our deployment time from 2 hours to 5 to 7 minutes because of CodeDeploy and trying to parallelly deploy all of the Auto Scaling groups at the same time.

[00:20:10] The third challenge that I talked about was the graceful shutdown during spot and during the scale-down. This was a challenge because we started hitting the AWS API limits when we were trying to use AWS APIs to identify if Auto Scaling groups have suggested a scale-down event. We started using EventBridge and Lambda for the same, and it brought it down.

[00:20:34] These are two specific stories that I want to go into detail on. I assume that most of you folks who might have used Celery in production might resonate with this. The first one is how to handle long-running tasks. This has been a challenge for a lot of folks who have been using asynchronous tasks and Celery specifically.

[00:20:57] Before that, I want to go into detail of a couple of terms that are used in Celery. There are two terms that are quite interesting to know. One is concurrency, which I already spoke about. Concurrency is nothing but the number of tasks that can be run in parallel by a particular worker, and that could be set based on the task itself and the nature of the worker that you have.

[00:21:31] The second is prefetch multiplier. This prefetch multiplier is a knob that is used. The way Celery works is that if you have a prefetch multiplier of one and a concurrency of 10, it's going to eventually pull 20 tasks or 20 messages from the queue: 10 which are acknowledged, 10 which will go unacknowledged. They'll just go ahead and sit in the Celery worker. They do it for obvious reasons, for performance benefit, because the round-trip time to pull the data from the queue is going to be huge. They say, "Let me prefetch these messages, keep them unacknowledged, and I'll process them once the first 10 goes through or whenever I have a free time."

[00:22:19] It works well for short tasks, but it doesn't work as well for long-running tasks. Now there is no direct setting in Celery to tell them that, "Do not prefetch." Prefetch zero basically means that it will continue to prefetch as much as it wants. The only way to manage it is by setting something called task_acks_late true. What it means: instead of acknowledging a task as soon as it accepts it, it's going to wait until it finishes processing to acknowledge it, and set the prefetch multiplier to one. It means that it is just going to take in only as many tasks as it can process.

[00:23:06] The advantage of this is going to be that if there are any other workers who are available and free to process the tasks that are on the queue, they're available for them, and it could just go around and get processed rather than sitting unacknowledged in a Celery worker and not getting processed while there are other workers that could bring it through. I've put in a link for some of these, how it is done. You could go into detail.

[00:23:31] The other knob that is quite interesting and something that could be quite powerful is visibility timeout. This is a knob on the queue, not in Celery itself. What visibility timeout basically means is that it's the time that the broker will hold the message until it's deleted, and it's not going to give it to some other worker to process.

[00:23:58] In short, all of these 10 messages that you see that are unacknowledged, they will remain in the queue but not be given to any other worker unless and until they themselves don't get acknowledged by the worker itself. It's possible that in case of a long-running event, these 10 tasks, having a small visibility timeout of probably 5 minutes, and if your worker itself is taking more than 5 minutes to process some of these events, the same task might be given to some other worker. That could cause some sort of integrity issues.

[00:24:36] The idea of visibility timeout is to find that sweet spot. That is why I think one of the earlier dashboards that I showed about identifying what is your P99 or max time taken for a particular event actually helps you understand what is the visibility timeout that you need to use and you need to tune on your broker itself. The simple math that we use is the max time taken plus add a buffer of 2 or 3 minutes so that you would never have a situation where you have duplicate messages going in and out to multiple workers.

[00:25:17] Those are a couple of decent war stories that we have encountered and we have overcome. Coming to our current challenges: there are a fair bit of challenges that we still have. Celery is quite vast, and a lot of people would say that the more you do, the more you learn.

[00:25:45] We're still trying to optimize ways to handle incomplete tasks due to cyclic spot workers and the prefetch stash handling. There are still those corner cases and events where these prefetch tasks do not get managed. We have been able to use cron and retries to get through it, but we wanted to get to a spot where anytime if there are any workers that are not able to process a task for one reason or the other for spot termination, there is a way to go back and trace that message: "Oh, this never finished processing and it needs to go back to the queue." That is something that's a challenge that we still haven't been able to solve.

[00:26:26] The other challenge obviously is profiling of Celery tasks. We have been trying to use Celery or an equivalent tool, but given the volume at which we operate, using any other tools like DataDog, Sentry itself, it becomes cumbersome to look through all of that data.

[00:26:48] Scaling of workers using Docker: that is something that we have tried, and at a small scale we have already done that. One of the challenges of using scaling with Docker is the scale-out event. Like I mentioned, when things scale out, you need to gracefully tell the Celery worker, "You need to stop accepting something new." That itself is a challenge in Docker because you need to send a SIGTERM event to tell Docker, "You need to stop accepting something new." That's a challenge we haven't been able to solve yet.

[00:27:27] Last but not least, as I mentioned, we have been trying to use Celery Prometheus and Flower to monitor the workers and the tasks itself, but due to challenges of SQS and the lack of support of SQS in combo itself, we haven't been able to use that. We might eventually build something on our own, but that continues to remain.

[00:27:55] I hope with all of this what I wanted to try and give you is a framework to help you understand how you could run Celery at scale in production. These are some of our experiences that we have tried. There is no substitute for doing this on your own and getting your hands dirty, but it gives you a starting point. I'll open up for questions if there are any.